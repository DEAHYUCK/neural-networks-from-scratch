{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfb09c22",
   "metadata": {},
   "source": [
    "# Chapter 12: Validation data\n",
    "\n",
    "In the chapter on optimization, we used hyperparameter tuning to select hyperparameters that lead to better results, but one more thing requires clarification.\n",
    "\n",
    "We should not check different hyperparameters using the test dataset, if we do that, we’re going to be manually optimizing the model to the test dataset, biasing it towards overfitting these data, and these data are supposed to be used only to perform the last check if the model trains and generalizes well.\n",
    "\n",
    "In other words, if we’re tuning our network’s parameters to fit the testing data, then we’re essentially optimizing our network on the testing data, which is another way for overfitting on these data.\n",
    "\n",
    "Thus, hyperparameter tuning using the test dataset is a mistake.\n",
    "\n",
    "The test dataset should only be used as unseen data.\n",
    "\n",
    "Hyperparameter tuning can be performed using yet another dataset called validation data.\n",
    "\n",
    "The test dataset needs to contain real out-of-sample data, but with a validation dataset, we have more freedom with choosing data.\n",
    "\n",
    "If we have a lot of training data and can afford to use some for validation purposes, we can take it as an out-of-sample dataset, similar to a test dataset. \n",
    "\n",
    "We can now search for parameters that work best using this new validation dataset and test our model at the end using the test dataset to see if we really tuned the model or just overfitted it to the validation data.\n",
    "\n",
    "<b>If we are short on data:</b>\n",
    "\n",
    "- The 1st option, temporarily split the training data into a smaller training and validation datasets for hyperparameter tuning. Then, train the model with the finial hyperparameter set on all the training data. We still have a test dataset to check the model's performance after training.\n",
    "\n",
    "- The 2nd option, k-fold cross-validation is primarily used when we have a small training dataset and cannot afford any data for validation purposes. We split the training dataset into a given number of parts (k), train the model on (k-1) chunks and validate it on the rest.\n",
    "\n",
    "<center><img src='./image/12-1.png' style='width: 70%'/><font color='gray'><i>Cross-validation, first step.</i></font></center>\n",
    "\n",
    "<center><img src='./image/12-2.png' style='width: 70%'/><font color='gray'><i>Cross-validation, third step.</i></font></center>\n",
    "\n",
    "When using a validation dataset and cross-validation, it is common to loop over different hyperparameter sets, leaving the code to run training multiple times, applying different settings each run, and reviewing the results to choose the best set of hyperparameters.\n",
    "\n",
    "In general, we should not loop over all possible setting combinations that we would like to check unless training is exceptionally fast.\n",
    "\n",
    "It’s usually better to check some settings that we suspect will work well, pick the best combination of those settings, tweak them to create the next list of setting sets, and train the model on new sets. We can repeat this process as many times as we’d like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1974bf95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
