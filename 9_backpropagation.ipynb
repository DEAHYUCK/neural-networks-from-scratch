{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35618caa",
   "metadata": {},
   "source": [
    "# Chapter 9: Backpropagation\n",
    "\n",
    "Start with a simplified forward pass with just one neuron.\n",
    "\n",
    "Let’s backpropagate the ReLU function for a single neuron, then intend to minimize the output for this single neuron as a practice to show how we can leverage the chain rule with derivatives and partial derivatives.\n",
    "\n",
    "We will start by <b>minimizing this more basic output</b> before jumping to the full network and overall loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b28f55da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0\n"
     ]
    }
   ],
   "source": [
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "xw0 = x[0] * w[0]\n",
    "print(xw0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df959429",
   "metadata": {},
   "source": [
    "<center><img src='./image/9-1.png' style='width: 70%'/><font color='gray'><i>The first input and weight multiplication.</i></font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76e5102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.0 6.0\n"
     ]
    }
   ],
   "source": [
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "print(xw0, xw1, xw2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdfb5fe",
   "metadata": {},
   "source": [
    "<center><img src='./image/9-2.png' style='width: 70%'/><font color='gray'><i>Input and weight multiplication of all of the inputs.</i></font></center>\n",
    "\n",
    "Perform a sum of all weighted inputs with a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781f8435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b7973",
   "metadata": {},
   "source": [
    "<center><img src='./image/9-3.png' style='width: 70%'/><font color='gray'><i>Weighted inputs and bias addition.</i></font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b0fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e897955",
   "metadata": {},
   "source": [
    "<center><img src='./image/9-4.png' style='width: 70%'/><font color='gray'><i>ReLU activation applied to the neuron output.</i></font></center>\n",
    "\n",
    "This is the full forward pass through a single neuron and a ReLU activation function.\n",
    "\n",
    "Let’s treat all of these chained functions as one big function which takes input values ($x$), weights ($w$), and bias ($b$), as inputs, and outputs ($y$).\n",
    "\n",
    "This big function consists of <b>3 chained functions</b> in total: a multiplication of input values and weights, a sum of these values and bias, as well as a $max$ function as the ReLU activation.\n",
    "\n",
    "\n",
    "## 9.1. Derivative of activation function\n",
    "\n",
    "Backpropagate our gradients by calculating derivatives and partial derivatives w.r.t each of our parameters and inputs.\n",
    "\n",
    "The big function in the context of our neural network, can be loosely interpreted as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU} \\Big( \\sum[ \\ \\text{input} \\cdot \\text{weights} \\ ] + \\text{bias} \\Big)\n",
    "$$\n",
    "\n",
    "Or in the form that matches code more precisely as:\n",
    "\n",
    "$$\n",
    "\\text{ReLU} \\Big( x_0 w_0 + x_1 w_1 + x_2 w_2 + b  \\Big)\n",
    "$$\n",
    "\n",
    "Rewrite the function to the form that will allow us to determine how to calculate the derivatives more easily:\n",
    "\n",
    "$$\n",
    "y = \\text{ReLU} \\Big( sum \\big( mul(x_0 , w_0 ), mul(x_1 , w_1 ), mul(x_2 , w_2 ), b \\big) \\Big)\n",
    "$$\n",
    "\n",
    "Calculate the partial derivative with respect to $w_0$.\n",
    "\n",
    "$$\n",
    "\\frac{∂}{∂x_0} \\bigg[ \\text{ReLU} \\Big( sum \\big( mul(x_0 , w_0 ), mul(x_1 , w_1 ), mul(x_2 , w_2 ), b \\big) \\Big)  \\bigg]\n",
    "$$\n",
    "\n",
    "The derivative with respect to the layer’s inputs is not used to update any parameters. It is used to chain to another layer.\n",
    "\n",
    "We can repeat this to calculate all of the other remaining impacts.\n",
    "\n",
    "We want to know the impact of a given weight or bias on the loss. Thus, we have to calculate the derivative of the loss function and apply the chain rule with the derivatives of all activation functions and neurons in all of the consecutive layers.\n",
    "\n",
    "Assume that our neuron receives a gradient of $1$ from the next layer (for demonstration purposes), a value of $1$ won't change the values, that means we can more easily show all of the processes. The color red is used for derivatives.\n",
    "\n",
    "<center><img src='./image/9-5.png' style='width: 70%'/><font color='gray'><i>Initial gradient (received during backpropagation).</i></font></center>\n",
    "\n",
    "Recall the derivative of $ReLU()$ w.r.t its input:\n",
    "\n",
    "$$\n",
    "f(x) = max(x,0) \\quad \\to \\quad \\frac{d}{dx} f(x) = 1 (x>0)\n",
    "$$\n",
    "\n",
    "The input value to the $ReLU$ function is $6$, so the derivative equals $1$.\n",
    "\n",
    "We have to use the chain rule and multiply this derivative with the derivative received from the next layer (which is 1 for the purpose of this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce3fe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "\n",
    "# Backward pass\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(drelu_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb864c3",
   "metadata": {},
   "source": [
    "<center><img src='./image/9-6.png' style='width: 70%'/><font color='gray'><i>Derivative of the $ReLU$ function and chain rule.</i></font></center>\n",
    "\n",
    "This results with the derivative of 1 :\n",
    "\n",
    "<center><img src='./image/9-7.png' style='width: 70%'/><font color='gray'><i>ReLU and chain rule gradient.</i></font></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2bec78",
   "metadata": {},
   "source": [
    "## 9.2. Derivative of a sum of the weighted inputs and bias\n",
    "\n",
    "Moving backward through our neural network, a sum of the weighted inputs and bias comes immediately before we perform the activation function.\n",
    "\n",
    "Thus, we must calculate the partial derivative of the sum function, then, using the chain rule, multiply this by the partial derivative of the subsequent, outer, function, which is $ReLU$.\n",
    "\n",
    "Recall the partial derivative of the sum operation is always 1:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y) = x+y \\quad \\to \\quad & \\frac{∂}{∂x} f(x,y) = 1\\\\\n",
    "& \\frac{∂}{∂y} f(x,y) = 1 \\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c276ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "\n",
    "# Backward pass\n",
    "# The derivative from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# Derivative of ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(drelu_dz)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a7db60",
   "metadata": {},
   "source": [
    "<center><img src='./image/9-8.png' style='width: 70%'/><font color='gray'><i>Partial derivative of the sum function w.r.t. the first weighted input.</i></font></center>\n",
    "\n",
    "This results with a partial derivative of 1 again:\n",
    "\n",
    "<center><img src='./image/9-9.png' style='width: 70%'/><font color='gray'><i>The sum and chain rule gradient for the first weighted input.</i></font></center>\n",
    "\n",
    "Perform the same operation with the next weighted input:\n",
    "\n",
    "<center><img src='./image/9-10.png' style='width: 70%'/><font color='gray'><i>Partial derivative of the sum function w.r.t. the second weighted input.</i></font></center>\n",
    "\n",
    "Which results with the next calculated partial derivative:\n",
    "\n",
    "<center><img src='./image/9-11.png' style='width: 70%'/><font color='gray'><i>The sum and chain rule gradient (for the second weighted input).</i></font></center>\n",
    "\n",
    "And the last weighted input:\n",
    "\n",
    "<center><img src='./image/9-12.png' style='width: 70%'/><font color='gray'><i>Partial derivative of the sum function w.r.t. the third weighted input.</i></font></center>\n",
    "\n",
    "<center><img src='./image/9-13.png' style='width: 70%'/><font color='gray'><i>The sum and chain rule gradient (for the third weighted input).</i></font></center>\n",
    "\n",
    "Then the bias:\n",
    "\n",
    "<center><img src='./image/9-14.png' style='width: 70%'/><font color='gray'><i>Partial derivative of the sum function w.r.t. the bias.</i></font></center>\n",
    "\n",
    "<center><img src='./image/9-15.png' style='width: 70%'/><font color='gray'><i>The sum and chain rule gradient (for the bias).</i></font></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd73a2",
   "metadata": {},
   "source": [
    "## 9.3. Derivative of the multiplication of weights and inputs\n",
    "\n",
    "The derivative for a product is whatever the input is being multiplied by.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x,y) = x \\cdot y \\quad \\to \\quad & \\frac{∂}{∂x} f(x,y) = y \\\\\n",
    "& \\frac{∂}{∂y} f(x,y) = x \\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246f2955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d9543",
   "metadata": {},
   "source": [
    "<center><img src='./image/9-16.png' style='width: 70%'/><font color='gray'><i>Partial derivative of the multiplication function w.r.t. the first input.</i></font></center>\n",
    "\n",
    "<center><img src='./image/9-17.png' style='width: 70%'/><font color='gray'><i>The multiplication and chain rule gradient (for the first input).</i></font></center>\n",
    "\n",
    "The complete set of the activated neuron’s partial derivatives with respect to the inputs, weights and a bias.\n",
    "\n",
    "<center><img src='./image/9-18.png' style='width: 70%'/><font color='gray'><i>Complete backpropagation graph.</i></font></center>\n",
    "\n",
    "Recall the equation from the beginning:\n",
    "\n",
    "$$\n",
    "\\frac{∂}{∂x_0} \\bigg[ \\text{ReLU} \\Big( sum \\big( mul(x_0 , w_0 ), mul(x_1 , w_1 ), mul(x_2 , w_2 ), b \\big) \\Big)  \\bigg]\n",
    "= \\frac{d \\text{ReLU} }{d sum()} \\cdot \\frac{∂ sum() }{∂ mul(x_0 , w_0 )} \\cdot \\frac{∂ mul(x_0 , w_0 )}{∂x_0}\n",
    "$$\n",
    "\n",
    "The partial derivative of a neuron’s function, with respect to the weight, is the input related to this weight, and, with respect to the input, is the related weight. The partial derivative of the neuron’s function with respect to the bias is always 1.\n",
    "\n",
    "\n",
    "## 9.4. Update weight and bias to decrease output\n",
    "\n",
    "All partial derivatives above combined into a vector, make up our gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a3fd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2] # gradients on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2] # gradients on weights\n",
    "db = drelu_db # gradient on bias...just 1 bias here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff74a24",
   "metadata": {},
   "source": [
    "For this single neuron example, we won't need our $dx$.\n",
    "\n",
    "We will apply these gradients to the weights to hopefully minimize the output.\n",
    "\n",
    "We apply a negative fraction to this gradient to decrease the final output value, as the gradient shows the direction of the steepest ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "431c089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39725f35",
   "metadata": {},
   "source": [
    "Apply a fraction of the gradients to these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19b25533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.001, -0.998, 1.997] 0.999\n"
     ]
    }
   ],
   "source": [
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412137f7",
   "metadata": {},
   "source": [
    "We slightly changed the weights and bias in such a way to decrease the output intelligently.\n",
    "\n",
    "Do another forward pass to see the effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b01d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.985\n"
     ]
    }
   ],
   "source": [
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dcfaee",
   "metadata": {},
   "source": [
    "We've successfully decreased this neuron's output from $6.000$ to $5.985$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb92a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
