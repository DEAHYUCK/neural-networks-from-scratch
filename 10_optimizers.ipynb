{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59085b91",
   "metadata": {},
   "source": [
    "# Chapter 10: Optimizers\n",
    "\n",
    "Once we have calculated the gradient, we can use this information to adjust weights and biases to decrease the measure of loss.\n",
    "\n",
    "## 10.1. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "SGD is still a commonly used optimizer. Most optimizers are just variants of SGD.\n",
    "\n",
    "<b>Stochastic Gradient Descent</b> is an optimizer that fits a single sample at a time.\n",
    "\n",
    "<b>Batch Gradient Descent</b> is an optimizer that fits a whole dataset at once.\n",
    "\n",
    "<b>Mini-batch Gradient Descent</b> is an optimizer that fits slices (batches) of a dataset.\n",
    "\n",
    "Current naming trends and conventions with Stochastic Gradient Descent in use with deep learning today have merged and normalized all of these variants, such that a batch of data can be a single sample, every sample in a dataset, or some subset of the full dataset at a time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d71abfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0\n",
      "-3.0 2.0 6.0\n",
      "6.0\n",
      "6.0\n",
      "1.0\n",
      "1.0\n",
      "1.0 1.0 1.0 1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n",
      "[-3.0, -1.0, 2.0] 1.0\n",
      "[-3.001, -0.998, 1.997] 0.999\n",
      "5.985\n",
      "[[ 0.44  0.44  0.44]\n",
      " [-0.38 -0.38 -0.38]\n",
      " [-0.07 -0.07 -0.07]\n",
      " [ 1.37  1.37  1.37]]\n",
      "[ 0.44 -0.38 -0.07  1.37]\n",
      "[[ 0.44 -0.38 -0.07  0.5 ]\n",
      " [ 0.88 -0.76 -0.14  1.  ]\n",
      " [ 1.32 -1.14 -0.21  1.5 ]]\n",
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n",
      "[[6. 6. 6.]]\n",
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n",
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n",
      "[[ 0.21 -0.07 -0.14]\n",
      " [-0.07  0.09 -0.02]\n",
      " [-0.14 -0.02  0.16]]\n",
      "Gradients: combined loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients: separate loss and activation:\n",
      "[[-0.09999999  0.03333334  0.06666667]\n",
      " [ 0.03333334 -0.16666667  0.13333334]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "5.016675980671116\n",
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104\n",
      "acc: 0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Layer_Dense' object has no attribute 'dweights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer_SGD()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Update our network layer's parameters after calculating the gradient\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdense1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate_params(dense2)\n",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m, in \u001b[0;36mOptimizer_SGD.update_params\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_params\u001b[39m (\u001b[38;5;28mself\u001b[39m, layer):    \u001b[38;5;66;03m# layer object contains its parameters (weights and biases) and also, at this stage, the gradient that is calculated during backpropagation\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     layer\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdweights\u001b[49m\n\u001b[0;32m     12\u001b[0m     layer\u001b[38;5;241m.\u001b[39mbiases \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m layer\u001b[38;5;241m.\u001b[39mdbiases\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Layer_Dense' object has no attribute 'dweights'"
     ]
    }
   ],
   "source": [
    "# To use all class, functions defined in another notebook:\n",
    "%run 9_backpropagation.ipynb\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialization method will take hyper-parameters, starting with learning_rate with default value of 1.\n",
    "    def __init__ (self, learning_rate = 1.0):\n",
    "        self.learning_rate = learning_rate      # storing hyper-parameters in the class' properties\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params (self, layer):    # layer object contains its parameters (weights and biases) and also, at this stage, the gradient that is calculated during backpropagation\n",
    "        layer.weights += - self.learning_rate * layer.dweights\n",
    "        layer.biases += - self.learning_rate * layer.dbiases\n",
    "\n",
    "        \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create an optimizer object\n",
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "\n",
    "# --------------------- A forward pass --------------------------\n",
    "\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Perform a forward pass through activation function, takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer, takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function, takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# Let's print loss value\n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets, calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "\n",
    "if len (y.shape) == 2 :\n",
    "    y = np.argmax(y, axis = 1)\n",
    "\n",
    "accuracy = np.mean(predictions == y)\n",
    "print ('acc:' , accuracy)\n",
    "\n",
    "\n",
    "# --------------------- A backward pass (backpropagation) --------------------------\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "\n",
    "# --------------------- Use optimizer to update weights and biases --------------------------\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e70a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
