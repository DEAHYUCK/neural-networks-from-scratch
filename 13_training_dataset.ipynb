{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e0581c",
   "metadata": {},
   "source": [
    "# Chapter 13: Training dataset\n",
    "\n",
    "Operations we perform on the training dataset is referred to as preprocessing.\n",
    "\n",
    "It’s important to remember that <b>any preprocessing we do to our training data also needs to be done to our validation and testing data and later done to the prediction data</b>.\n",
    "\n",
    "Neural networks usually perform best on data consisting of numbers in a range of 0 to 1 or -1 to 1, with the latter being preferable.\n",
    "\n",
    "Two reasons:\n",
    "\n",
    "- We multiply data by a weight and sum the results with a bias, we're usually passing the resulting output to an activation function. Many activation functions behave properly within this described range. For example, softmax outputs a vector of probabilities containing numbers in the range of 0 to 1; sigmoid also has an output range of 0 to 1, but tanh outputs a range from -1 to 1.\n",
    "\n",
    "- NN's reliance on many multiplication operations. If we multiply by numbers above 1 or below -1, the resulting value is larger in scale than the original one, that might cause floating-point overflow or instability — weights growing too fast. Within the -1 to 1 range, the result becomes a fraction, a smaller value. It’s easier to control the training process with smaller numbers. It’s easier to control the training process with smaller numbers.\n",
    "\n",
    "There are many terms related to data preprocessing: standardization, scaling, variance scaling, mean removal, non-linear transformations, scaling to outliers, etc.\n",
    "\n",
    "We need to ensure identical scaling for all the datasets (same scale parameters).\n",
    "\n",
    "In cases where we do not have many training samples, we could use data augmentation .\n",
    "\n",
    "One easy way to understand augmentation is in the case of images.\n",
    "\n",
    "Let’s imagine that our model’s goal is to detect rotten fruits — apples, for example. We will take a photo of an apple from different angles and predict whether it’s rotten. We should get more pictures in this case, but let’s assume that we cannot. What we could do is to take photos that we have, rotate, crop, and save those as worthy data too. This way, we have added more samples to the dataset, which can help with model generalization.\n",
    "\n",
    "In general, if we use augmentation, then it’s only useful if the augmentations that we make are similar to variations that we could see in reality.\n",
    "\n",
    "<b>How many samples do we need to train the model?</b>\n",
    "\n",
    "There is no single answer to this question — one model might require just a few per class, and another may require a few million or billion.\n",
    "\n",
    "Usually, a few thousand per class will be necessary, and a few tens of thousands should be preferable to start.\n",
    "\n",
    "The difference depends on the data complexity and model size.\n",
    "\n",
    "- If the model has to predict sensor data with 2 simple classes, for example, if an image contains a dark area or does not, hundreds of samples per class might be enough.\n",
    "\n",
    "- To train on data with many features and several classes, tens of thousands of samples are what you should start with.\n",
    "\n",
    "- If you’re attempting to train a chatbot the intricacies of written language, then you’re going to likely want at least millions of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa362b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
